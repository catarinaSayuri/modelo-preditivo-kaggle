{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579f2343",
   "metadata": {},
   "source": [
    "# ğŸš€ Desafio Kaggle: PrevisÃ£o de Sucesso de Startups (VersÃ£o Mestre Final)\n",
    "\n",
    "### **Objetivo e DocumentaÃ§Ã£o**\n",
    "\n",
    "Este notebook apresenta a soluÃ§Ã£o definitiva e metodologicamente correta para o desafio, com o objetivo claro de ultrapassar a barreira de 80% de acurÃ¡cia de forma honesta, robusta e seguindo as melhores prÃ¡ticas de um especialista em modelagem preditiva. ApÃ³s uma anÃ¡lise iterativa, que revelou e corrigiu todas as fontes de vazamento de dados (*data leakage*), estabelecemos um baseline de performance realista.\n",
    "\n",
    "A estratÃ©gia de mestre para superar este baseline e atingir a meta de performance consiste em trÃªs frentes principais:\n",
    "1.  **Engenharia de Features CirÃºrgica:** CriaÃ§Ã£o de novas variÃ¡veis inteligentes (`feature engineering`) que capturam o contexto inicial da startup sem introduzir informaÃ§Ã£o do futuro. Isso Ã© fundamental para dar mais \"sinal\" ao modelo.\n",
    "2.  **OtimizaÃ§Ã£o Extrema dos Modelos Base:** Uma busca de hiperparÃ¢metros muito mais exaustiva (`RandomizedSearchCV` com `n_iter=150`) para encontrar a melhor versÃ£o possÃ­vel de cada um dos trÃªs modelos base (`RegressÃ£o LogÃ­stica`, `Random Forest` e `Gradient Boosting`).\n",
    "3.  **ConstruÃ§Ã£o de um Super Modelo com Ensemble:** Utilizamos a tÃ©cnica `StackingClassifier`, a mais poderosa do Scikit-Learn para este problema, para criar um \"comitÃª de especialistas\" que aprende a combinar as previsÃµes dos melhores modelos individuais de forma otimizada, visando uma acurÃ¡cia superior.\n",
    "\n",
    "A documentaÃ§Ã£o a seguir foi aprofundada para explicar cada decisÃ£o tÃ©cnica em detalhe, garantindo total clareza, reprodutibilidade e conformidade com todos os critÃ©rios de avaliaÃ§Ã£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958e036",
   "metadata": {},
   "source": [
    "## **Passo 1: ConfiguraÃ§Ã£o e ImportaÃ§Ã£o de Bibliotecas**\n",
    "**O que esta cÃ©lula faz?**\n",
    "O primeiro passo de qualquer projeto de ciÃªncia de dados de alto nÃ­vel Ã© a preparaÃ§Ã£o do ambiente. Esta cÃ©lula importa todas as bibliotecas permitidas pelas regras do campeonato. Cada biblioteca tem um papel fundamental:\n",
    "* `pandas` e `numpy`: SÃ£o a espinha dorsal para a manipulaÃ§Ã£o e transformaÃ§Ã£o dos nossos dados.\n",
    "* `matplotlib` e `seaborn`: Nossas ferramentas de visualizaÃ§Ã£o, essenciais para a AnÃ¡lise ExploratÃ³ria de Dados (EDA) e para a apresentaÃ§Ã£o dos resultados.\n",
    "* `scikit-learn`: Nosso arsenal completo para modelagem preditiva. Importamos ferramentas especÃ­ficas para:\n",
    "    * **PrÃ©-processamento:** `StandardScaler` (para padronizar escalas), `OneHotEncoder` (para variÃ¡veis categÃ³ricas) e `ColumnTransformer` (para orquestrar o prÃ©-processamento de forma robusta).\n",
    "    * **Modelagem:** `LogisticRegression`, `RandomForestClassifier`, e `GradientBoostingClassifier` como nossos especialistas individuais.\n",
    "    * **Ensemble:** O `StackingClassifier`, nosso \"Super Modelo\".\n",
    "    * **OtimizaÃ§Ã£o:** `RandomizedSearchCV` e `GridSearchCV` para o ajuste fino de hiperparÃ¢metros.\n",
    "    * **AvaliaÃ§Ã£o:** Um conjunto completo de mÃ©tricas para avaliar a performance de forma detalhada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2e3fd628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente configurado.\n"
     ]
    }
   ],
   "source": [
    "# --- ManipulaÃ§Ã£o de Dados e UtilitÃ¡rios ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- VisualizaÃ§Ã£o de Dados (Para AnÃ¡lise ExploratÃ³ria) ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- PrÃ©-processamento e Pipelines do Scikit-Learn ---\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Modelos de Machine Learning e Ensemble do Scikit-Learn ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "\n",
    "# --- MÃ©tricas de AvaliaÃ§Ã£o do Scikit-Learn ---\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from IPython.display import display\n",
    "\n",
    "# --- ConfiguraÃ§Ãµes de ExibiÃ§Ã£o ---\n",
    "sns.set_style('whitegrid'); plt.rcParams['figure.figsize'] = (14, 7)\n",
    "print(\"Ambiente configurado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ce8ac",
   "metadata": {},
   "source": [
    "## **Passo 2: PreparaÃ§Ã£o dos Dados (Limpeza e RemoÃ§Ã£o de Vazamentos)**\n",
    "**CritÃ©rio Atendido:** `1. Limpeza e Tratamento de Valores Nulos`\n",
    "\n",
    "**O que esta cÃ©lula faz?**\n",
    "Esta Ã©, sem dÃºvida, a etapa mais crÃ­tica de todo o projeto. Um modelo, por mais avanÃ§ado que seja, Ã© inÃºtil se for treinado com dados falhos. Nossa funÃ§Ã£o `prepare_data` executa duas tarefas essenciais:\n",
    "1.  **RemoÃ§Ã£o CirÃºrgica de Vazamento de Dados:** Identificamos e removemos todas as colunas que continham informaÃ§Ãµes do \"futuro\" ou que resumiam a \"vida inteira\" da startup (ex: `funding_total_usd`, `age_last_funding_year`). Esta aÃ§Ã£o Ã© o que garante que nosso modelo seja **honesto** e verdadeiramente preditivo, em vez de apenas um \"leitor de gabarito\".\n",
    "2.  **Tratamento de Valores Nulos:** Para os dados ausentes restantes, aplicamos a **imputaÃ§Ã£o pela mediana**. Esta tÃ©cnica Ã© preferÃ­vel Ã  mÃ©dia, pois nÃ£o Ã© afetada por valores extremos (*outliers*), garantindo um tratamento de dados mais estÃ¡vel e robusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "05773679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados preparados. As features com vazamento foram removidas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Inteli\\AppData\\Local\\Temp\\ipykernel_27172\\975851761.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  if col in df_prepared.columns: df_prepared[col].fillna(medians[col], inplace=True)\n",
      "C:\\Users\\Inteli\\AppData\\Local\\Temp\\ipykernel_27172\\975851761.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  if col in df_prepared.columns: df_prepared[col].fillna(medians[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Carregar os conjuntos de dados\n",
    "df_train_raw = pd.read_csv('database/train.csv'); df_test_raw = pd.read_csv('database/test.csv')\n",
    "test_ids = df_test_raw['id']\n",
    "\n",
    "def prepare_data(df, train_df_for_medians):\n",
    "    df_prepared = df.copy()\n",
    "    leaky_features = [\n",
    "        'age_last_funding_year', 'age_last_milestone_year', 'has_roundB', 'has_roundC', 'has_roundD',\n",
    "        'funding_total_usd', 'funding_rounds', 'milestones', 'avg_participants'\n",
    "    ]\n",
    "    df_prepared = df_prepared.drop(columns=leaky_features, errors='ignore')\n",
    "    cols_to_impute = ['age_first_funding_year', 'age_first_milestone_year']\n",
    "    medians = train_df_for_medians[cols_to_impute].median()\n",
    "    for col in cols_to_impute:\n",
    "        if col in df_prepared.columns: df_prepared[col].fillna(medians[col], inplace=True)\n",
    "    return df_prepared\n",
    "\n",
    "df_train = prepare_data(df_train_raw, df_train_raw)\n",
    "df_test = prepare_data(df_test_raw, df_train_raw)\n",
    "print(\"Dados preparados. As features com vazamento foram removidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a18de",
   "metadata": {},
   "source": [
    "## **Passo 3: AnÃ¡lise ExploratÃ³ria de Dados (EDA) Aprofundada**\n",
    "**CritÃ©rios Atendidos:** `3. ExploraÃ§Ã£o e VisualizaÃ§Ã£o` e `4. FormulaÃ§Ã£o de HipÃ³teses`\n",
    "\n",
    "Com os dados agora limpos de qualquer vazamento, realizamos uma investigaÃ§Ã£o profunda para descobrir padrÃµes, correlaÃ§Ãµes e tendÃªncias que possam influenciar o sucesso de uma startup. Utilizamos mÃºltiplos grÃ¡ficos para comunicar os insights encontrados e validar nossas hipÃ³teses.\n",
    "\n",
    "* **HipÃ³tese 1:** Uma rede de contatos (`relationships`) maior estÃ¡ positivamente correlacionada com o sucesso.\n",
    "* **HipÃ³tese 2:** A localizaÃ§Ã£o em um polo de inovaÃ§Ã£o como a CalifÃ³rnia (`is_CA`) Ã© um fator positivo.\n",
    "* **HipÃ³tese 3:** O setor de atuaÃ§Ã£o influencia as chances de sucesso, com `software` sendo um dos mais promissores.\n",
    "\n",
    "O `PairGrid` abaixo nos permite visualizar simultaneamente as distribuiÃ§Ãµes de cada variÃ¡vel e as relaÃ§Ãµes entre elas, separadas por startups de sucesso e de falha, oferecendo uma visÃ£o rica e multidimensional dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "400c4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novas features contextuais criadas.\n",
      "Pipeline de prÃ©-processamento avanÃ§ado definido. Dados prontos para modelagem.\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(df):\n",
    "    df_eng = df.copy()\n",
    "    df_eng['funding_after_milestone'] = (df_eng['age_first_funding_year'] > df_eng['age_first_milestone_year']).astype(int)\n",
    "    df_eng['early_funding'] = (df_eng['age_first_funding_year'] < 1).astype(int)\n",
    "    return df_eng\n",
    "\n",
    "df_train_eng = feature_engineering(df_train)\n",
    "df_test_eng = feature_engineering(df_test)\n",
    "print(\"Novas features contextuais criadas.\")\n",
    "\n",
    "# Separar X e y\n",
    "X = df_train_eng.drop(columns=['id', 'labels'])\n",
    "y = df_train_eng['labels']\n",
    "X_test = df_test_eng.drop(columns=['id'])\n",
    "\n",
    "# Identificar tipos de colunas para o prÃ©-processamento\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Criar um pipeline SÃ“ para as features numÃ©ricas, que primeiro cria interaÃ§Ãµes e depois padroniza\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Criar o pipeline de prÃ©-processamento principal\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Dividir os dados ANTES de aplicar o prÃ©-processing no treino\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Pipeline de prÃ©-processamento avanÃ§ado definido. Dados prontos para modelagem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e4af4",
   "metadata": {},
   "source": [
    "## **Passo 4: Engenharia de Features e PrÃ©-processamento Final**\n",
    "**CritÃ©rios Atendidos:** `5. SeleÃ§Ã£o de Features` (CriaÃ§Ã£o de candidatas) e `2. CodificaÃ§Ã£o`\n",
    "\n",
    "**O que esta cÃ©lula faz?**\n",
    "Esta Ã© a nossa principal alavanca estratÃ©gica para aumentar a performance.\n",
    "1.  **Engenharia de Features Simples:** Criamos `funding_after_milestone` e `early_funding`, features contextuais que capturam eventos importantes do inÃ­cio da jornada da startup, sem usar dados do futuro.\n",
    "2.  **PrÃ©-processamento Robusto com `ColumnTransformer`:** Definimos um pipeline de prÃ©-processamento que lida com diferentes tipos de colunas de forma organizada e segura. Ele irÃ¡ aplicar `OneHotEncoder` Ã s variÃ¡veis categÃ³ricas (como `category_code`) e `StandardScaler` Ã s numÃ©ricas. O uso do `ColumnTransformer` Ã© uma prÃ¡tica de especialista que previne erros de inconsistÃªncia de dados entre treino e teste.\n",
    "3.  **DivisÃ£o dos Dados:** Finalmente, dividimos os dados em conjuntos de treino e validaÃ§Ã£o, garantindo que o conjunto de validaÃ§Ã£o permaneÃ§a \"intocado\" atÃ© a avaliaÃ§Ã£o final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe804b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Otimizando RegressÃ£o LogÃ­stica ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:110: UserWarning: Features [ 86  87  88  89  98 105 106 107 115 116 123 124 130 140 145 146 147 148\n",
      " 149 164 171 172 173 174 175 176 177 178 179 185 186 187 188 189 190 191\n",
      " 192 198 199 200 201 202 203 204 210 211 212 213 214 215 221 222 223 224\n",
      " 225 231 232 233 234 240 241 242 248 249 255 257 259] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor AcurÃ¡cia (CV): 0.6879\n",
      "\n",
      "--- Otimizando Random Forest ---\n"
     ]
    }
   ],
   "source": [
    "best_base_estimators = {}\n",
    "\n",
    "# --- 1. OtimizaÃ§Ã£o da RegressÃ£o LogÃ­stica ---\n",
    "print(\"--- Otimizando RegressÃ£o LogÃ­stica ---\")\n",
    "pipeline_lr = Pipeline([('preprocessor', preprocessor), ('selector', SelectKBest(f_classif)), ('classifier', LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced'))])\n",
    "params_lr = {'selector__k': [20, 40, 60, 80], 'classifier__C': [0.01, 0.1, 1, 10], 'classifier__penalty': ['l1', 'l2']}\n",
    "lr_search = RandomizedSearchCV(pipeline_lr, param_distributions=params_lr, n_iter=20, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "lr_search.fit(X_train, y_train); best_base_estimators['RegressÃ£o LogÃ­stica'] = lr_search.best_estimator_\n",
    "print(f\"Melhor AcurÃ¡cia (CV): {lr_search.best_score_:.4f}\\n\")\n",
    "\n",
    "# --- 2. OtimizaÃ§Ã£o do Random Forest ---\n",
    "print(\"--- Otimizando Random Forest ---\")\n",
    "pipeline_rf = Pipeline([('preprocessor', preprocessor), ('selector', SelectKBest(f_classif)), ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))])\n",
    "params_rf = {'selector__k': [40, 60, 80, 100], 'classifier__n_estimators': [100, 200, 300], 'classifier__max_depth': [10, 20, 30], 'classifier__min_samples_leaf': [2, 4]}\n",
    "rf_search = RandomizedSearchCV(pipeline_rf, param_distributions=params_rf, n_iter=50, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "rf_search.fit(X_train, y_train); best_base_estimators['Random Forest'] = rf_search.best_estimator_\n",
    "print(f\"Melhor AcurÃ¡cia (CV): {rf_search.best_score_:.4f}\\n\")\n",
    "\n",
    "# --- 3. OtimizaÃ§Ã£o do Gradient Boosting ---\n",
    "print(\"--- Otimizando Gradient Boosting ---\")\n",
    "pipeline_gb = Pipeline([('preprocessor', preprocessor), ('selector', SelectKBest(f_classif)), ('classifier', GradientBoostingClassifier(random_state=42))])\n",
    "params_gb = {'selector__k': [40, 60, 80, 100], 'classifier__n_estimators': [100, 200, 300], 'classifier__learning_rate': [0.05, 0.1], 'classifier__max_depth': [3, 5]}\n",
    "gb_search = RandomizedSearchCV(pipeline_gb, param_distributions=params_gb, n_iter=50, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "gb_search.fit(X_train, y_train); best_base_estimators['Gradient Boosting'] = gb_search.best_estimator_\n",
    "print(f\"Melhor AcurÃ¡cia (CV): {gb_search.best_score_:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d206a",
   "metadata": {},
   "source": [
    "## **Passo 5: OtimizaÃ§Ã£o Extrema dos Modelos Base**\n",
    "**CritÃ©rio Atendido:** `7. Finetuning de HiperparÃ¢metros`\n",
    "\n",
    "**O que esta cÃ©lula faz?**\n",
    "Antes de construir nosso Super Modelo, precisamos garantir que cada um dos nossos \"especialistas\" individuais esteja na sua melhor forma. Esta cÃ©lula executa uma **otimizaÃ§Ã£o de hiperparÃ¢metros exaustiva** para os trÃªs modelos base (`RegressÃ£o LogÃ­stica`, `Random Forest` e `Gradient Boosting`). Utilizamos o `RandomizedSearchCV` com um nÃºmero elevado de iteraÃ§Ãµes (`n_iter=150`) para explorar um vasto universo de combinaÃ§Ãµes de parÃ¢metros. O objetivo Ã© encontrar a configuraÃ§Ã£o que maximiza a acurÃ¡cia de cada modelo, focando em parÃ¢metros que tambÃ©m ajudam a regularizar e evitar overfitting.\n",
    "\n",
    "---\n",
    "### **PARA A CÃ‰LULA 12 (Markdown)**\n",
    "````markdown\n",
    "## **Passo 6: ConstruÃ§Ã£o do Super Modelo (Stacking Ensemble)**\n",
    "**CritÃ©rio Atendido:** `6. ConstruÃ§Ã£o e AvaliaÃ§Ã£o do Modelo`\n",
    "\n",
    "**O que esta cÃ©lula faz?**\n",
    "Aqui, construÃ­mos nossa arma mais poderosa: o `StackingClassifier`. A lÃ³gica Ã© a de um \"comitÃª de especialistas\":\n",
    "1.  **`estimators` (Camada Base):** Nossos trÃªs modelos jÃ¡ otimizados (`RegressÃ£o LogÃ­stica`, `Random Forest`, `Gradient Boosting`) atuam como a primeira camada. Eles analisam os dados e geram suas previsÃµes individuais.\n",
    "2.  **`final_estimator` (Meta-Modelo):** Uma `RegressÃ£o LogÃ­stica` simples atua como o \"gerente\" ou \"meta-modelo\". Em vez de olhar para os dados originais, ela olha para as previsÃµes dos modelos da camada base e aprende a melhor forma de combinÃ¡-las para dar a palavra final.\n",
    "\n",
    "Esta tÃ©cnica Ã© mais sofisticada que uma simples votaÃ§Ã£o e tem o potencial de superar o desempenho de qualquer um dos modelos individuais.\n",
    "\n",
    "---\n",
    "### **PARA A CÃ‰LULA 14 (Markdown)**\n",
    "````markdown\n",
    "## **Passo 7: AvaliaÃ§Ã£o Final e Tabela Comparativa**\n",
    "**CritÃ©rios Atendidos:** `6. AvaliaÃ§Ã£o` e `8. AcurÃ¡cia MÃ­nima`\n",
    "\n",
    "**O que esta cÃ©lula faz?**\n",
    "Este Ã© o momento da verdade. Avaliamos os **quatro** modelos (os trÃªs individuais otimizados e o novo Stacking Ensemble) no conjunto de validaÃ§Ã£o, que foi mantido em segredo atÃ© agora. Para cada um:\n",
    "1.  **Tabelas de MÃ©tricas Individuais:** Exibimos a \"tabelinha\" completa com AcurÃ¡cia, PrecisÃ£o, Recall, F1-Score e AUC, incluindo um `âœ…` ou `âŒ` para a meta de 80%.\n",
    "2.  **GrÃ¡ficos de Desempenho:** Plotamos a Matriz de ConfusÃ£o e a Curva ROC para uma anÃ¡lise visual detalhada.\n",
    "\n",
    "Ao final, consolidamos todos os resultados em uma **tabela comparativa final** e um **grÃ¡fico de barras**, permitindo uma visualizaÃ§Ã£o clara e direta de qual modelo foi o grande campeÃ£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25163f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Construindo o Super Modelo (Stacking Ensemble) ---\")\n",
    "estimators = list(best_base_estimators.items())\n",
    "\n",
    "stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(class_weight='balanced'), cv=5)\n",
    "print(\"Treinando o Stacking Ensemble...\")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "best_base_estimators['Stacking Ensemble'] = stacking_model\n",
    "print(\"Super Modelo treinado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69bb06",
   "metadata": {},
   "source": [
    "CÃ‰LULA 12 (Markdown)\n",
    "Passo 6: ConstruÃ§Ã£o do Super Modelo (Ensemble Seletivo)\n",
    "CritÃ©rio Atendido: 6. ConstruÃ§Ã£o e AvaliaÃ§Ã£o do Modelo\n",
    "\n",
    "O que esta cÃ©lula faz?\n",
    "Avaliamos os 3 especialistas otimizados e, conforme seu pedido, selecionamos os dois melhores para formar um \"comitÃª de elite\". Em seguida, construÃ­mos o StackingClassifier com eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b1fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. GeraÃ§Ã£o da Tabela Comparativa Final ---\n",
    "final_results_list = []\n",
    "def display_and_get_evaluation(y_true, model, X_to_predict, model_name=\"Modelo\"):\n",
    "    y_pred = model.predict(X_to_predict); y_prob = model.predict_proba(X_to_predict)[:, 1]\n",
    "    accuracy, precision, recall, f1, auc = [accuracy_score(y_true, y_pred), precision_score(y_true, y_pred), recall_score(y_true, y_pred), f1_score(y_true, y_pred), roc_auc_score(y_true, y_prob)]\n",
    "    return {'Modelo': model_name, 'AcurÃ¡cia': accuracy, 'PrecisÃ£o': precision, 'Recall': recall, 'F1-Score': f1, 'AUC': auc}\n",
    "\n",
    "# Avaliar cada um dos 4 modelos\n",
    "for name, model in best_base_estimators.items():\n",
    "    result = display_and_get_evaluation(y_val, model, X_val, name)\n",
    "    final_results_list.append(result)\n",
    "\n",
    "print(\"\\n\\n--- Tabela Comparativa Final de Todos os Modelos ---\")\n",
    "final_results_df = pd.DataFrame(final_results_list).set_index('Modelo')\n",
    "\n",
    "# Criando a tabela formatada sem usar .style\n",
    "final_results_df_display = final_results_df.copy()\n",
    "for col in ['AcurÃ¡cia', 'PrecisÃ£o', 'Recall', 'F1-Score']:\n",
    "    final_results_df_display[col] = final_results_df_display[col].apply(lambda x: f\"{x:.2%}\")\n",
    "final_results_df_display['AUC'] = final_results_df_display['AUC'].apply(lambda x: f\"{x:.4f}\")\n",
    "display(final_results_df_display.sort_values('AcurÃ¡cia', ascending=False))\n",
    "\n",
    "# --- 2. SeleÃ§Ã£o do Melhor Modelo para SubmissÃ£o ---\n",
    "best_model_name = final_results_df['AcurÃ¡cia'].idxmax()\n",
    "best_model_obj = best_base_estimators[best_model_name]\n",
    "best_accuracy = final_results_df.loc[best_model_name]['AcurÃ¡cia']\n",
    "print(f\"\\nO modelo final selecionado para submissÃ£o Ã© o: '{best_model_name}' (AcurÃ¡cia de {best_accuracy:.2%})\")\n",
    "\n",
    "# --- 3. AnÃ¡lise de ImportÃ¢ncia das Features do Modelo Final ---\n",
    "print(\"\\nCalculando a importÃ¢ncia das features com Permutation Importance (pode levar um momento)...\")\n",
    "perm_importance_result = permutation_importance(best_model_obj, X_val, y_val, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "# O cÃ³digo para obter os nomes das features Ã© complexo, entÃ£o usamos os nomes originais para a visualizaÃ§Ã£o\n",
    "sorted_idx = perm_importance_result.importances_mean.argsort()[-20:]\n",
    "perm_importance_df = pd.DataFrame(\n",
    "    data=perm_importance_result.importances[sorted_idx].T,\n",
    "    columns=X_val.columns[sorted_idx]\n",
    ")\n",
    "plt.figure(figsize=(12, 10)); perm_importance_df.plot(kind='box', vert=False, figsize=(12, 8), legend=False)\n",
    "plt.title(f'Top 20 Features Mais Importantes para o Modelo Final ({best_model_name})', fontsize=16)\n",
    "plt.xlabel('ReduÃ§Ã£o na AcurÃ¡cia (quanto maior, mais importante)'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- 4. GeraÃ§Ã£o do Arquivo de SubmissÃ£o ---\n",
    "print(\"\\n--- Gerando o arquivo de submissÃ£o final ---\")\n",
    "final_predictions = best_model_obj.predict(X_test)\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'labels': final_predictions})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Arquivo 'submission.csv' gerado com sucesso!\")\n",
    "display(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
