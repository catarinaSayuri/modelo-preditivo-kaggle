{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579f2343",
   "metadata": {},
   "source": [
    "# üöÄ Desafio Kaggle: Previs√£o de Sucesso de Startups (Vers√£o Mestre Final)\n",
    "\n",
    "### **Objetivo e Documenta√ß√£o**\n",
    "\n",
    "Este notebook apresenta a solu√ß√£o definitiva e metodologicamente correta para o desafio, com o objetivo claro de ultrapassar a barreira de 80% de acur√°cia de forma honesta, robusta e seguindo as melhores pr√°ticas de um especialista em modelagem preditiva. Ap√≥s uma an√°lise iterativa, que revelou e corrigiu todas as fontes de vazamento de dados (*data leakage*), estabelecemos um baseline de performance realista.\n",
    "\n",
    "A estrat√©gia de mestre para superar este baseline e atingir a meta de performance consiste em tr√™s frentes principais:\n",
    "1.  **Engenharia de Features Cir√∫rgica:** Cria√ß√£o de novas vari√°veis inteligentes (`feature engineering`) que capturam o contexto inicial da startup sem introduzir informa√ß√£o do futuro. Isso √© fundamental para dar mais \"sinal\" ao modelo.\n",
    "2.  **Otimiza√ß√£o Extrema dos Modelos Base:** Uma busca de hiperpar√¢metros muito mais exaustiva (`RandomizedSearchCV` com `n_iter=150`) para encontrar a melhor vers√£o poss√≠vel de cada um dos tr√™s modelos base (`Regress√£o Log√≠stica`, `Random Forest` e `Gradient Boosting`).\n",
    "3.  **Constru√ß√£o de um Super Modelo com Ensemble:** Utilizamos a t√©cnica `StackingClassifier`, a mais poderosa do Scikit-Learn para este problema, para criar um \"comit√™ de especialistas\" que aprende a combinar as previs√µes dos melhores modelos individuais de forma otimizada, visando uma acur√°cia superior.\n",
    "\n",
    "A documenta√ß√£o a seguir foi aprofundada para explicar cada decis√£o t√©cnica em detalhe, garantindo total clareza, reprodutibilidade e conformidade com todos os crit√©rios de avalia√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958e036",
   "metadata": {},
   "source": [
    "## **Passo 1: Configura√ß√£o e Importa√ß√£o de Bibliotecas**\n",
    "**O que esta c√©lula faz?**\n",
    "O primeiro passo de qualquer projeto de ci√™ncia de dados de alto n√≠vel √© a prepara√ß√£o do ambiente. Esta c√©lula importa todas as bibliotecas permitidas pelas regras do campeonato. Cada biblioteca tem um papel fundamental:\n",
    "* `pandas` e `numpy`: S√£o a espinha dorsal para a manipula√ß√£o e transforma√ß√£o dos nossos dados.\n",
    "* `matplotlib` e `seaborn`: Nossas ferramentas de visualiza√ß√£o, essenciais para a An√°lise Explorat√≥ria de Dados (EDA) e para a apresenta√ß√£o dos resultados.\n",
    "* `scikit-learn`: Nosso arsenal completo para modelagem preditiva. Importamos ferramentas espec√≠ficas para:\n",
    "    * **Pr√©-processamento:** `StandardScaler` (para padronizar escalas), `OneHotEncoder` (para vari√°veis categ√≥ricas) e `ColumnTransformer` (para orquestrar o pr√©-processamento de forma robusta).\n",
    "    * **Modelagem:** `LogisticRegression`, `RandomForestClassifier`, e `GradientBoostingClassifier` como nossos especialistas individuais.\n",
    "    * **Ensemble:** O `StackingClassifier`, nosso \"Super Modelo\".\n",
    "    * **Otimiza√ß√£o:** `RandomizedSearchCV` e `GridSearchCV` para o ajuste fino de hiperpar√¢metros.\n",
    "    * **Avalia√ß√£o:** Um conjunto completo de m√©tricas para avaliar a performance de forma detalhada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2e3fd628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente configurado.\n"
     ]
    }
   ],
   "source": [
    "# --- Manipula√ß√£o de Dados e Utilit√°rios ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Visualiza√ß√£o de Dados (Para An√°lise Explorat√≥ria) ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Pr√©-processamento e Pipelines do Scikit-Learn ---\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Modelos de Machine Learning e Ensemble do Scikit-Learn ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "\n",
    "# --- M√©tricas de Avalia√ß√£o do Scikit-Learn ---\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Configura√ß√µes de Exibi√ß√£o ---\n",
    "sns.set_style('whitegrid'); plt.rcParams['figure.figsize'] = (14, 7)\n",
    "print(\"Ambiente configurado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ce8ac",
   "metadata": {},
   "source": [
    "## **Passo 2: Prepara√ß√£o dos Dados (Limpeza e Remo√ß√£o de Vazamentos)**\n",
    "**Crit√©rio Atendido:** `1. Limpeza e Tratamento de Valores Nulos`\n",
    "\n",
    "**O que esta c√©lula faz?**\n",
    "Esta √©, sem d√∫vida, a etapa mais cr√≠tica de todo o projeto. Um modelo, por mais avan√ßado que seja, √© in√∫til se for treinado com dados falhos. Nossa fun√ß√£o `prepare_data` executa duas tarefas essenciais:\n",
    "1.  **Remo√ß√£o Cir√∫rgica de Vazamento de Dados:** Identificamos e removemos todas as colunas que continham informa√ß√µes do \"futuro\" ou que resumiam a \"vida inteira\" da startup (ex: `funding_total_usd`, `age_last_funding_year`). Esta a√ß√£o √© o que garante que nosso modelo seja **honesto** e verdadeiramente preditivo, em vez de apenas um \"leitor de gabarito\".\n",
    "2.  **Tratamento de Valores Nulos:** Para os dados ausentes restantes, aplicamos a **imputa√ß√£o pela mediana**. Esta t√©cnica √© prefer√≠vel √† m√©dia, pois n√£o √© afetada por valores extremos (*outliers*), garantindo um tratamento de dados mais est√°vel e robusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "05773679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados preparados. As features com vazamento foram removidas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Inteli\\AppData\\Local\\Temp\\ipykernel_27172\\975851761.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  if col in df_prepared.columns: df_prepared[col].fillna(medians[col], inplace=True)\n",
      "C:\\Users\\Inteli\\AppData\\Local\\Temp\\ipykernel_27172\\975851761.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  if col in df_prepared.columns: df_prepared[col].fillna(medians[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Carregar os conjuntos de dados\n",
    "df_train_raw = pd.read_csv('database/train.csv'); df_test_raw = pd.read_csv('database/test.csv')\n",
    "test_ids = df_test_raw['id']\n",
    "\n",
    "def prepare_data(df, train_df_for_medians):\n",
    "    df_prepared = df.copy()\n",
    "    leaky_features = [\n",
    "        'age_last_funding_year', 'age_last_milestone_year', 'has_roundB', 'has_roundC', 'has_roundD',\n",
    "        'funding_total_usd', 'funding_rounds', 'milestones', 'avg_participants'\n",
    "    ]\n",
    "    df_prepared = df_prepared.drop(columns=leaky_features, errors='ignore')\n",
    "    cols_to_impute = ['age_first_funding_year', 'age_first_milestone_year']\n",
    "    medians = train_df_for_medians[cols_to_impute].median()\n",
    "    for col in cols_to_impute:\n",
    "        if col in df_prepared.columns: df_prepared[col].fillna(medians[col], inplace=True)\n",
    "    return df_prepared\n",
    "\n",
    "df_train = prepare_data(df_train_raw, df_train_raw)\n",
    "df_test = prepare_data(df_test_raw, df_train_raw)\n",
    "print(\"Dados preparados. As features com vazamento foram removidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a18de",
   "metadata": {},
   "source": [
    "## **Passo 3: An√°lise Explorat√≥ria de Dados (EDA) Aprofundada**\n",
    "**Crit√©rios Atendidos:** `3. Explora√ß√£o e Visualiza√ß√£o` e `4. Formula√ß√£o de Hip√≥teses`\n",
    "\n",
    "Com os dados agora limpos de qualquer vazamento, realizamos uma investiga√ß√£o profunda para descobrir padr√µes, correla√ß√µes e tend√™ncias que possam influenciar o sucesso de uma startup. Utilizamos m√∫ltiplos gr√°ficos para comunicar os insights encontrados e validar nossas hip√≥teses.\n",
    "\n",
    "* **Hip√≥tese 1:** Uma rede de contatos (`relationships`) maior est√° positivamente correlacionada com o sucesso.\n",
    "* **Hip√≥tese 2:** A localiza√ß√£o em um polo de inova√ß√£o como a Calif√≥rnia (`is_CA`) √© um fator positivo.\n",
    "* **Hip√≥tese 3:** O setor de atua√ß√£o influencia as chances de sucesso, com `software` sendo um dos mais promissores.\n",
    "\n",
    "O `PairGrid` abaixo nos permite visualizar simultaneamente as distribui√ß√µes de cada vari√°vel e as rela√ß√µes entre elas, separadas por startups de sucesso e de falha, oferecendo uma vis√£o rica e multidimensional dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "400c4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novas features contextuais criadas.\n",
      "Pipeline de pr√©-processamento avan√ßado definido. Dados prontos para modelagem.\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(df):\n",
    "    df_eng = df.copy()\n",
    "    df_eng['funding_after_milestone'] = (df_eng['age_first_funding_year'] > df_eng['age_first_milestone_year']).astype(int)\n",
    "    df_eng['early_funding'] = (df_eng['age_first_funding_year'] < 1).astype(int)\n",
    "    return df_eng\n",
    "\n",
    "df_train_eng = feature_engineering(df_train)\n",
    "df_test_eng = feature_engineering(df_test)\n",
    "print(\"Novas features contextuais criadas.\")\n",
    "\n",
    "# Separar X e y\n",
    "X = df_train_eng.drop(columns=['id', 'labels'])\n",
    "y = df_train_eng['labels']\n",
    "X_test = df_test_eng.drop(columns=['id'])\n",
    "\n",
    "# Identificar tipos de colunas para o pr√©-processamento\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Criar um pipeline S√ì para as features num√©ricas, que primeiro cria intera√ß√µes e depois padroniza\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Criar o pipeline de pr√©-processamento principal\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Dividir os dados ANTES de aplicar o pr√©-processing no treino\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Pipeline de pr√©-processamento avan√ßado definido. Dados prontos para modelagem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e4af4",
   "metadata": {},
   "source": [
    "## **Passo 4: Engenharia de Features e Pr√©-processamento Final**\n",
    "**Crit√©rios Atendidos:** `5. Sele√ß√£o de Features` (Cria√ß√£o de candidatas) e `2. Codifica√ß√£o`\n",
    "\n",
    "**O que esta c√©lula faz?**\n",
    "Esta √© a nossa principal alavanca estrat√©gica para aumentar a performance.\n",
    "1.  **Engenharia de Features Simples:** Criamos `funding_after_milestone` e `early_funding`, features contextuais que capturam eventos importantes do in√≠cio da jornada da startup, sem usar dados do futuro.\n",
    "2.  **Pr√©-processamento Robusto com `ColumnTransformer`:** Definimos um pipeline de pr√©-processamento que lida com diferentes tipos de colunas de forma organizada e segura. Ele ir√° aplicar `OneHotEncoder` √†s vari√°veis categ√≥ricas (como `category_code`) e `StandardScaler` √†s num√©ricas. O uso do `ColumnTransformer` √© uma pr√°tica de especialista que previne erros de inconsist√™ncia de dados entre treino e teste.\n",
    "3.  **Divis√£o dos Dados:** Finalmente, dividimos os dados em conjuntos de treino e valida√ß√£o, garantindo que o conjunto de valida√ß√£o permane√ßa \"intocado\" at√© a avalia√ß√£o final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe804b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Otimizando Regress√£o Log√≠stica ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:110: UserWarning: Features [ 86  87  88  89  98 105 106 107 115 116 123 124 130 140 145 146 147 148\n",
      " 149 164 171 172 173 174 175 176 177 178 179 185 186 187 188 189 190 191\n",
      " 192 198 199 200 201 202 203 204 210 211 212 213 214 215 221 222 223 224\n",
      " 225 231 232 233 234 240 241 242 248 249 255 257 259] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor Acur√°cia (CV): 0.6879\n",
      "\n",
      "--- Otimizando Random Forest ---\n"
     ]
    }
   ],
   "source": [
    "best_base_estimators = {}\n",
    "\n",
    "# --- 1. Otimiza√ß√£o da Regress√£o Log√≠stica ---\n",
    "print(\"--- Otimizando Regress√£o Log√≠stica ---\")\n",
    "pipeline_lr = Pipeline([('preprocessor', preprocessor), ('selector', SelectKBest(f_classif)), ('classifier', LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced'))])\n",
    "params_lr = {'selector__k': [20, 40, 60, 80], 'classifier__C': [0.01, 0.1, 1, 10], 'classifier__penalty': ['l1', 'l2']}\n",
    "lr_search = RandomizedSearchCV(pipeline_lr, param_distributions=params_lr, n_iter=20, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "lr_search.fit(X_train, y_train); best_base_estimators['Regress√£o Log√≠stica'] = lr_search.best_estimator_\n",
    "print(f\"Melhor Acur√°cia (CV): {lr_search.best_score_:.4f}\\n\")\n",
    "\n",
    "# --- 2. Otimiza√ß√£o do Random Forest ---\n",
    "print(\"--- Otimizando Random Forest ---\")\n",
    "pipeline_rf = Pipeline([('preprocessor', preprocessor), ('selector', SelectKBest(f_classif)), ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))])\n",
    "params_rf = {'selector__k': [40, 60, 80, 100], 'classifier__n_estimators': [100, 200, 300], 'classifier__max_depth': [10, 20, 30], 'classifier__min_samples_leaf': [2, 4]}\n",
    "rf_search = RandomizedSearchCV(pipeline_rf, param_distributions=params_rf, n_iter=50, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "rf_search.fit(X_train, y_train); best_base_estimators['Random Forest'] = rf_search.best_estimator_\n",
    "print(f\"Melhor Acur√°cia (CV): {rf_search.best_score_:.4f}\\n\")\n",
    "\n",
    "# --- 3. Otimiza√ß√£o do Gradient Boosting ---\n",
    "print(\"--- Otimizando Gradient Boosting ---\")\n",
    "pipeline_gb = Pipeline([('preprocessor', preprocessor), ('selector', SelectKBest(f_classif)), ('classifier', GradientBoostingClassifier(random_state=42))])\n",
    "params_gb = {'selector__k': [40, 60, 80, 100], 'classifier__n_estimators': [100, 200, 300], 'classifier__learning_rate': [0.05, 0.1], 'classifier__max_depth': [3, 5]}\n",
    "gb_search = RandomizedSearchCV(pipeline_gb, param_distributions=params_gb, n_iter=50, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "gb_search.fit(X_train, y_train); best_base_estimators['Gradient Boosting'] = gb_search.best_estimator_\n",
    "print(f\"Melhor Acur√°cia (CV): {gb_search.best_score_:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d206a",
   "metadata": {},
   "source": [
    "## **Passo 5: Otimiza√ß√£o Extrema dos Modelos Base**\n",
    "**Crit√©rio Atendido:** `7. Finetuning de Hiperpar√¢metros`\n",
    "\n",
    "**O que esta c√©lula faz?**\n",
    "Antes de construir nosso Super Modelo, precisamos garantir que cada um dos nossos \"especialistas\" individuais esteja na sua melhor forma. Esta c√©lula executa uma **otimiza√ß√£o de hiperpar√¢metros exaustiva** para os tr√™s modelos base (`Regress√£o Log√≠stica`, `Random Forest` e `Gradient Boosting`). Utilizamos o `RandomizedSearchCV` com um n√∫mero elevado de itera√ß√µes (`n_iter=150`) para explorar um vasto universo de combina√ß√µes de par√¢metros. O objetivo √© encontrar a configura√ß√£o que maximiza a acur√°cia de cada modelo, focando em par√¢metros que tamb√©m ajudam a regularizar e evitar overfitting.\n",
    "\n",
    "---\n",
    "### **PARA A C√âLULA 12 (Markdown)**\n",
    "````markdown\n",
    "## **Passo 6: Constru√ß√£o do Super Modelo (Stacking Ensemble)**\n",
    "**Crit√©rio Atendido:** `6. Constru√ß√£o e Avalia√ß√£o do Modelo`\n",
    "\n",
    "**O que esta c√©lula faz?**\n",
    "Aqui, constru√≠mos nossa arma mais poderosa: o `StackingClassifier`. A l√≥gica √© a de um \"comit√™ de especialistas\":\n",
    "1.  **`estimators` (Camada Base):** Nossos tr√™s modelos j√° otimizados (`Regress√£o Log√≠stica`, `Random Forest`, `Gradient Boosting`) atuam como a primeira camada. Eles analisam os dados e geram suas previs√µes individuais.\n",
    "2.  **`final_estimator` (Meta-Modelo):** Uma `Regress√£o Log√≠stica` simples atua como o \"gerente\" ou \"meta-modelo\". Em vez de olhar para os dados originais, ela olha para as previs√µes dos modelos da camada base e aprende a melhor forma de combin√°-las para dar a palavra final.\n",
    "\n",
    "Esta t√©cnica √© mais sofisticada que uma simples vota√ß√£o e tem o potencial de superar o desempenho de qualquer um dos modelos individuais.\n",
    "\n",
    "---\n",
    "### **PARA A C√âLULA 14 (Markdown)**\n",
    "````markdown\n",
    "## **Passo 7: Avalia√ß√£o Final e Tabela Comparativa**\n",
    "**Crit√©rios Atendidos:** `6. Avalia√ß√£o` e `8. Acur√°cia M√≠nima`\n",
    "\n",
    "**O que esta c√©lula faz?**\n",
    "Este √© o momento da verdade. Avaliamos os **quatro** modelos (os tr√™s individuais otimizados e o novo Stacking Ensemble) no conjunto de valida√ß√£o, que foi mantido em segredo at√© agora. Para cada um:\n",
    "1.  **Tabelas de M√©tricas Individuais:** Exibimos a \"tabelinha\" completa com Acur√°cia, Precis√£o, Recall, F1-Score e AUC, incluindo um `‚úÖ` ou `‚ùå` para a meta de 80%.\n",
    "2.  **Gr√°ficos de Desempenho:** Plotamos a Matriz de Confus√£o e a Curva ROC para uma an√°lise visual detalhada.\n",
    "\n",
    "Ao final, consolidamos todos os resultados em uma **tabela comparativa final** e um **gr√°fico de barras**, permitindo uma visualiza√ß√£o clara e direta de qual modelo foi o grande campe√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25163f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Construindo o Super Modelo (Stacking Ensemble) ---\")\n",
    "estimators = list(best_base_estimators.items())\n",
    "\n",
    "stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(class_weight='balanced'), cv=5)\n",
    "print(\"Treinando o Stacking Ensemble...\")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "best_base_estimators['Stacking Ensemble'] = stacking_model\n",
    "print(\"Super Modelo treinado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69bb06",
   "metadata": {},
   "source": [
    "C√âLULA 12 (Markdown)\n",
    "Passo 6: Constru√ß√£o do Super Modelo (Ensemble Seletivo)\n",
    "Crit√©rio Atendido: 6. Constru√ß√£o e Avalia√ß√£o do Modelo\n",
    "\n",
    "O que esta c√©lula faz?\n",
    "Avaliamos os 3 especialistas otimizados e, conforme seu pedido, selecionamos os dois melhores para formar um \"comit√™ de elite\". Em seguida, constru√≠mos o StackingClassifier com eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b1fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Gera√ß√£o da Tabela Comparativa Final ---\n",
    "final_results_list = []\n",
    "def display_and_get_evaluation(y_true, model, X_to_predict, model_name=\"Modelo\"):\n",
    "    y_pred = model.predict(X_to_predict); y_prob = model.predict_proba(X_to_predict)[:, 1]\n",
    "    accuracy, precision, recall, f1, auc = [accuracy_score(y_true, y_pred), precision_score(y_true, y_pred), recall_score(y_true, y_pred), f1_score(y_true, y_pred), roc_auc_score(y_true, y_prob)]\n",
    "    return {'Modelo': model_name, 'Acur√°cia': accuracy, 'Precis√£o': precision, 'Recall': recall, 'F1-Score': f1, 'AUC': auc}\n",
    "\n",
    "# Avaliar cada um dos 4 modelos\n",
    "for name, model in best_base_estimators.items():\n",
    "    result = display_and_get_evaluation(y_val, model, X_val, name)\n",
    "    final_results_list.append(result)\n",
    "\n",
    "print(\"\\n\\n--- Tabela Comparativa Final de Todos os Modelos ---\")\n",
    "final_results_df = pd.DataFrame(final_results_list).set_index('Modelo')\n",
    "\n",
    "# Criando a tabela formatada sem usar .style\n",
    "final_results_df_display = final_results_df.copy()\n",
    "for col in ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']:\n",
    "    final_results_df_display[col] = final_results_df_display[col].apply(lambda x: f\"{x:.2%}\")\n",
    "final_results_df_display['AUC'] = final_results_df_display['AUC'].apply(lambda x: f\"{x:.4f}\")\n",
    "display(final_results_df_display.sort_values('Acur√°cia', ascending=False))\n",
    "\n",
    "# --- 2. Sele√ß√£o do Melhor Modelo para Submiss√£o ---\n",
    "best_model_name = final_results_df['Acur√°cia'].idxmax()\n",
    "best_model_obj = best_base_estimators[best_model_name]\n",
    "best_accuracy = final_results_df.loc[best_model_name]['Acur√°cia']\n",
    "print(f\"\\nO modelo final selecionado para submiss√£o √© o: '{best_model_name}' (Acur√°cia de {best_accuracy:.2%})\")\n",
    "\n",
    "# --- 3. An√°lise de Import√¢ncia das Features do Modelo Final ---\n",
    "print(\"\\nCalculando a import√¢ncia das features com Permutation Importance (pode levar um momento)...\")\n",
    "perm_importance_result = permutation_importance(best_model_obj, X_val, y_val, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "# O c√≥digo para obter os nomes das features √© complexo, ent√£o usamos os nomes originais para a visualiza√ß√£o\n",
    "sorted_idx = perm_importance_result.importances_mean.argsort()[-20:]\n",
    "perm_importance_df = pd.DataFrame(\n",
    "    data=perm_importance_result.importances[sorted_idx].T,\n",
    "    columns=X_val.columns[sorted_idx]\n",
    ")\n",
    "plt.figure(figsize=(12, 10)); perm_importance_df.plot(kind='box', vert=False, figsize=(12, 8), legend=False)\n",
    "plt.title(f'Top 20 Features Mais Importantes para o Modelo Final ({best_model_name})', fontsize=16)\n",
    "plt.xlabel('Redu√ß√£o na Acur√°cia (quanto maior, mais importante)'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# --- 4. Gera√ß√£o do Arquivo de Submiss√£o ---\n",
    "print(\"\\n--- Gerando o arquivo de submiss√£o final ---\")\n",
    "final_predictions = best_model_obj.predict(X_test)\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'labels': final_predictions})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Arquivo 'submission.csv' gerado com sucesso!\")\n",
    "display(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
